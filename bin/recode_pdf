#!/usr/bin/env python3
import pkg_resources

import sys
import os
import subprocess
from os import remove
from time import time
from datetime import datetime
from tempfile import mkstemp
from os.path import join
import shutil
import json
from glob import glob
from math import ceil
import re

from PIL import Image
from PIL import Jpeg2KImagePlugin
import fitz

from internetarchivepdf.mrc import KDU_EXPAND, create_mrc_components, create_mrc_hocr_components, \
        encode_mrc_images
from internetarchivepdf.pdfrenderer import TessPDFRenderer
from internetarchivepdf.pagenumbers import parse_series, series_to_pdf
from internetarchivepdf.scandata import scandata_xml_get_skip_pages, \
        scandata_xml_get_page_numbers
from hocr.parse import (hocr_page_iterator, hocr_page_to_word_data,
        hocr_page_get_dimensions)


VERSION = '0.0.1'

# XXX: More stable URL?
SOFTWARE_URL = 'https://git.archive.org/merlijn/archive-pdf-tools'
PRODUCER = 'Internet Archive PDF creator and recoder %s; %s; written by '\
           'Merlijn B. W. Wajer. Powered by Tesseract, mupdf and '\
           'Python (pymupdf/skimage).' % (VERSION, SOFTWARE_URL)


STOP = None
VERBOSE = False
REPORT_EVERY = None
TMP_DIR = None

IMAGE_MODE_PASSTHROUGH = 0
IMAGE_MODE_PIXMAP = 1
IMAGE_MODE_MRC = 2


def create_tess_textonly_pdf(hocr_file, save_path, in_pdf=None,
        image_files=None, dpi=None, skip_pages=None, reporter=None):
    hocr_iter = hocr_page_iterator(hocr_file)

    render = TessPDFRenderer()
    render.BeginDocumentHandler()

    skipped_pages = 0

    last_time = time()
    reporting_page_count = 0

    for idx, hocr_page in enumerate(hocr_iter):
        w, h = hocr_page_get_dimensions(hocr_page)

        if skip_pages is not None and idx in skip_pages:
            if VERBOSE:
                print('Skipping page %d' % idx)
            skipped_pages += 1
            continue

        if STOP is not None and (idx - skipped_pages) >= STOP:
            break

        if in_pdf is not None:
            page = in_pdf[idx - skipped_pages]
            width = page.rect.width
            height = page.rect.height

            scaler = page.rect.width / w
            ppi = 72 / scaler
        elif image_files is not None:
            # Do not subtract skipped pages here
            imgfile = image_files[idx]

            img = Image.open(imgfile)
            imwidth, imheight = img.size

            page_width = imwidth / (dpi / 72)

            scaler = page_width / imwidth

            ppi = 72. / scaler

            width = page_width
            height = imheight * scaler


        word_data = hocr_page_to_word_data(hocr_page, scaler)
        render.AddImageHandler(word_data, width, height, ppi=ppi)

        reporting_page_count += 1


    if reporter and reporting_page_count != 0:
        current_time = time()
        ms = int(((current_time - last_time) / reporting_page_count) * 1000)

        data = json.dumps({'text_pages': {'count': reporting_page_count,
                                              'time-per': ms}})
        subprocess.check_output(reporter, input=data.encode('utf-8'))

    render.EndDocumentHandler()

    fp = open(save_path, 'wb+')
    fp.write(render._data)
    fp.close()


def insert_images_mrc(to_pdf, hocr_file, from_pdf=None, image_files=None,
        dpi=None, bg_slope=None, fg_slope=None,
        skip_pages=None, img_dir=None, jbig2=False, bg_downsample=None,
        denoise_mask=None, reporter=None,
        hq_pages=None, hq_bg_slope=None, hq_fg_slope=None):
    hocr_iter = hocr_page_iterator(hocr_file)

    skipped_pages = 0

    last_time = time()
    reporting_page_count = 0

    #for idx, page in enumerate(to_pdf):
    for idx, hocr_page in enumerate(hocr_iter):
        if skip_pages is not None and idx in skip_pages:
            if VERBOSE:
                print('IMAGES Skipping page %d' % idx)
            skipped_pages += 1
            continue

        idx = idx - skipped_pages

        if STOP is not None and idx >= STOP:
            break

        page = to_pdf[idx]

        if from_pdf is not None:
            # XXX: TODO: FIXME: MEGAHACK: For some reason the _imgonly PDFs
            # generated by us have all images on all pages according to pymupdf, so
            # hack around that for now.
            img = sorted(from_pdf.getPageImageList(idx))[idx]
            #img = from_pdf.getPageImageList(idx)[0]

            xref = img[0]
            maskxref = img[1]
            # TODO: Do not assume JPX/JPEG2000 here, probe for image format
            image = from_pdf.extractImage(xref)
            jpx = image["image"]

            fd, jpx_in = mkstemp(prefix='in', suffix='.jpx', dir=TMP_DIR)
            os.write(fd, jpx)
            os.close(fd)

            fd, tiff_in = mkstemp(prefix='in', suffix='.tiff', dir=TMP_DIR)
            os.close(fd)
            os.remove(tiff_in)

            subprocess.check_call([KDU_EXPAND, '-i', jpx_in, '-o',
                tiff_in], stderr=subprocess.DEVNULL,
                stdout=subprocess.DEVNULL)
            os.remove(jpx_in)

            image = Image.open(tiff_in)
        else:
            # Do not subtract skipped pages here
            imgfile = image_files[idx+skipped_pages]

            if imgfile.endswith('.jp2') or imgfile.endswith('.jpx'):
                fd, tiff_in = mkstemp(prefix='in', suffix='.tiff', dir=TMP_DIR)
                os.close(fd)
                os.remove(tiff_in)
                subprocess.check_call([KDU_EXPAND, '-i', imgfile, '-o',
                    tiff_in], stderr=subprocess.DEVNULL,
                    stdout=subprocess.DEVNULL)

                image = Image.open(tiff_in)
                image.load()
                os.remove(tiff_in)
            else:
                image = Image.open(imgfile)

        render_hq = hq_pages[idx]

        hocr_word_data = hocr_page_to_word_data(hocr_page)
        mask, bg, fg = create_mrc_hocr_components(image,
                                                  hocr_word_data,
                                                  bg_downsample=None if render_hq else bg_downsample,
                                                  denoise_mask=denoise_mask)
        if from_pdf is not None:
            remove(tiff_in)

        mask_f, bg_f, fg_f = encode_mrc_images(mask, bg, fg,
                bg_slope=hq_bg_slope if render_hq else bg_slope,
                fg_slope=hq_fg_slope if render_hq else fg_slope,
                tmp_dir=TMP_DIR, jbig2=jbig2)

        if img_dir is not None:
            shutil.copy(mask_f, join(img_dir, '%.6d_mask.jbig2' % idx))
            shutil.copy(bg_f, join(img_dir, '%.6d_bg.jp2' % idx))
            shutil.copy(fg_f, join(img_dir, '%.6d_fg.jp2' % idx))

        bg_contents = open(bg_f, 'rb').read()
        page.insertImage(page.rect, stream=bg_contents, mask=None,
                overlay=False)

        fg_contents = open(fg_f, 'rb').read()
        mask_contents = open(mask_f, 'rb').read()

        page.insertImage(page.rect, stream=fg_contents, mask=mask_contents,
                overlay=True)

        # Remove leftover files
        remove(mask_f)
        remove(bg_f)
        remove(fg_f)

        reporting_page_count += 1

        if REPORT_EVERY is not None and reporting_page_count % REPORT_EVERY == 0:
            print('Processed %d PDF pages.' % idx)
            sys.stdout.flush()

            if reporter:
                current_time = time()
                ms = int(((current_time - last_time) / reporting_page_count) * 1000)

                data = json.dumps({'compress_pages': {'count': reporting_page_count,
                                                 'time-per': ms}})
                subprocess.check_output(reporter, input=data.encode('utf-8'))

                # Reset chunk timer
                last_time = time()
                # Reset chunk counter
                reporting_page_count = 0


    if reporter and reporting_page_count != 0:
        current_time = time()
        ms = int(((current_time - last_time) / reporting_page_count) * 1000)

        data = json.dumps({'text_pages': {'count': reporting_page_count,
                                         'time-per': ms}})
        subprocess.check_output(reporter, input=data.encode('utf-8'))


def insert_images(from_pdf, to_pdf, mode):
    # TODO: implement img_dir here

    for idx, page in enumerate(to_pdf):
        # XXX: TODO: FIXME: MEGAHACK: For some reason the _imgonly PDFs
        # generated by us have all images on all pages according to pymupdf, so
        # hack around that for now.
        img = sorted(from_pdf.getPageImageList(idx))[idx]
        #img = from_pdf.getPageImageList(idx)[0]

        xref = img[0]
        maskxref = img[1]
        if mode == IMAGE_MODE_PASSTHROUGH:
            image = from_pdf.extractImage(xref)
            page.insertImage(page.rect, stream=image["image"], overlay=False)
        elif mode == IMAGE_MODE_PIXMAP:
            pixmap = fitz.Pixmap(from_pdf, xref)
            page.insertImage(page.rect, pixmap=pixmap, overlay=False)

        if STOP is not None and idx >= STOP:
            break

        if REPORT_EVERY is not None and idx % REPORT_EVERY == 0:
            print('Processed %d PDF pages.' % idx)
            sys.stdout.flush()


# XXX: tmp.icc - pick proper one and ship it with the tool, or embed it
def write_pdfa(to_pdf):
    srgbxref = to_pdf._getNewXref()
    to_pdf.updateObject(srgbxref, """
<<
      /Alternate /DeviceRGB
      /N 3
>>
""")
    icc = pkg_resources.resource_string('internetarchivepdf', "data/tmp.icc")
    to_pdf.updateStream(srgbxref, icc, new=True)

    intentxref = to_pdf._getNewXref()
    to_pdf.updateObject(intentxref, """
<<
  /Type /OutputIntent
  /S /GTS_PDFA1
  /OutputConditionIdentifier (Custom)
  /Info (sRGB IEC61966-2.1)
  /DestOutputProfile %d 0 R
>>
""" % srgbxref)

    catalogxref = to_pdf.PDFCatalog()
    s = to_pdf.xrefObject(to_pdf.PDFCatalog())
    s = s[:-2]
    s += '  /OutputIntents [ %d 0 R ]' % intentxref
    s += '>>'
    to_pdf.updateObject(catalogxref, s)


def write_page_labels(to_pdf, scandata):
    page_numbers = scandata_xml_get_page_numbers(scandata)
    res = parse_series(page_numbers)

    catalogxref = to_pdf.PDFCatalog()
    s = to_pdf.xrefObject(to_pdf.PDFCatalog())
    s = s[:-2]
    s += series_to_pdf(res)
    s += '>>'
    to_pdf.updateObject(catalogxref, s)



def write_basic_ua(to_pdf, language=None):
    # Create StructTreeRoot and descendants, allocate new xrefs as needed
    structtreeroot_xref = to_pdf._getNewXref()
    parenttree_xref = to_pdf._getNewXref()
    page_info_xrefs = []
    page_info_a_xrefs = []
    parenttree_kids_xrefs = []
    parenttree_kids_indirect_xrefs = []

    kids_cnt = ceil(to_pdf.pageCount / 32)
    for _ in range(kids_cnt):
        kid_xref = to_pdf._getNewXref()
        parenttree_kids_xrefs.append(kid_xref)

    # Parent tree contains a /Kids entry with a list of xrefs, that each contain
    # a list of xrefs (limited to 32 per), and each entry in that list of list
    # of xrefs contains a single reference that points to the page info xref.
    for idx, page in enumerate(to_pdf):
        page_info_xref = to_pdf._getNewXref()
        page_info_xrefs.append(page_info_xref)

        page_info_a_xref = to_pdf._getNewXref()
        page_info_a_xrefs.append(page_info_a_xref)

        parenttree_kids_indirect_xref = to_pdf._getNewXref()
        parenttree_kids_indirect_xrefs.append(parenttree_kids_indirect_xref)


    for idx in range(kids_cnt):
        start = idx*32
        stop = (idx+1)*31
        if stop > to_pdf.pageCount:
            stop = to_pdf.pageCount - 1

        s = """<<
  /Limits [ %d %d ]
""" % (start, stop - 1)
        s += '  /Nums [ '

        for pidx in range(start, stop):
            s += '%d %d 0 R ' % (pidx, parenttree_kids_indirect_xrefs[pidx])

            if idx % 7 == 0:
                s = s[:-1] + '\n' + '      '

        s += ']\n>>'

        to_pdf.updateObject(parenttree_kids_xrefs[idx], s)


    for idx, page in enumerate(to_pdf):
        intrect = tuple([int(x) for x in page.rect])

        s = """<<
  /BBox [ %d %d %d %d ]
  /InlineAlign /Center
  /O /Layout
  /Placement /Block
>>
""" % intrect
        to_pdf.updateObject(page_info_a_xrefs[idx], s)

        s = """ <<
  /A %d 0 R
  /K 0
  /P %d 0 R
  /Pg %d 0 R
  /S /Figure
>>""" % (page_info_a_xrefs[idx], structtreeroot_xref, page.xref)

        to_pdf.updateObject(page_info_xrefs[idx], s)


    for idx, page in enumerate(to_pdf):
        s = '[ %d 0 R ]' % page_info_a_xrefs[idx]
        to_pdf.updateObject(parenttree_kids_indirect_xrefs[idx], s)


    K = '  /Kids [ '
    for idx in range(kids_cnt):
        K += '%d 0 R ' % parenttree_kids_xrefs[idx]

        if idx % 7 == 0:
            K = K[:-1] + '\n' + '      '

    K += ']'
    s = """<<
%s
>>
""" % K

    to_pdf.updateObject(parenttree_xref, s)

    K = '  /K [ '
    for idx, xref in enumerate(page_info_xrefs):
        K += '%d 0 R ' % xref

        if idx % 7 == 0:
            K = K[:-1] + '\n' + '      '

    K += ']'

    to_pdf.updateObject(structtreeroot_xref, """
<<
""" + K + """
  /Type /StructTreeRoot
  /ParentTree %d 0 R
>>
""" % parenttree_xref)

    #  TODO? /ClassMap 1006 0 R
    #  TODO? /ParentTreeNextKey 198


    # Update pages, add back xrefs
    for idx, page in enumerate(to_pdf):
        page_data = to_pdf.xrefObject(page.xref)
        page_data = page_data[:-2]

        page_data += """
  /StructParents %d
""" % idx

        page_data += """
  /CropBox [ 0 0 %.1f %.1f ]
""" % (page.rect[2], page.rect[3])

        page_data += """
  /Rotate 0
"""
        page_data += """
  /Tabs /S
"""
        page_data += '>>'
        to_pdf.updateObject(page.xref, page_data)

    catalogxref = to_pdf.PDFCatalog()
    s = to_pdf.xrefObject(to_pdf.PDFCatalog())
    s = s[:-2]
    s += """
  /ViewerPreferences <<
    /FitWindow true
    /DisplayDocTitle true
  >>
"""
    if language:
        s += """
  /Lang (%s)
""" % language

    s += """
  /MarkInfo <<
    /Marked true
  >>
"""
    s += """
  /StructTreeRoot %d 0 R
""" % structtreeroot_xref

    s += '>>'
    to_pdf.updateObject(catalogxref, s)



def write_metadata(from_pdf, to_pdf, extra_metadata):
    doc_md = from_pdf.metadata if from_pdf is not None else {}

    doc_md['producer'] = PRODUCER

    if 'url' in extra_metadata:
        doc_md['keywords'] = extra_metadata['url']
    if 'title' in extra_metadata:
        doc_md['title'] = extra_metadata['title']
    if 'author' in extra_metadata:
        doc_md['author'] = extra_metadata['author']
    if 'creator' in extra_metadata:
        doc_md['creator'] = extra_metadata['creator']
    if 'subject' in extra_metadata:
        doc_md['subject'] = extra_metadata['subject']

    current_time = 'D:' + datetime.utcnow().strftime('%Y%m%d%H%M%SZ')
    if from_pdf is not None:
        doc_md['creationDate'] = from_pdf.metadata['creationDate']
    else:
        doc_md['creationDate'] = current_time
    doc_md['modDate'] = current_time

    # Set PDF basic metadata
    to_pdf.setMetadata(doc_md)

    have_xmlmeta = (from_pdf is not None) and (from_pdf._getXmlMetadataXref() > 0)
    if have_xmlmeta:
        xml_xref = from_pdf._getXmlMetadataXref()

        # Just copy the existing XML, perform no validity checks
        xml_bytes = from_pdf.xrefStream(xml_xref)
        to_pdf.setXmlMetadata(xml_bytes.decode('utf-8'))
    else:
        current_time = datetime.utcnow().isoformat(timespec='seconds') + 'Z'

        stream='''<?xpacket begin="" id="W5M0MpCehiHzreSzNTczkc9d"?>
        <x:xmpmeta xmlns:x="adobe:ns:meta/">
          <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
            <rdf:Description rdf:about="" xmlns:xmp="http://ns.adobe.com/xap/1.0/">
              <xmp:CreateDate>{createdate}</xmp:CreateDate>
              <xmp:MetadataDate>{metadatadate}</xmp:MetadataDate>
              <xmp:ModifyDate>{modifydate}</xmp:ModifyDate>
              <xmp:CreatorTool>{creatortool}</xmp:CreatorTool>
            </rdf:Description>
            <rdf:Description rdf:about="" xmlns:dc="http://purl.org/dc/elements/1.1/">'''.format(creatortool=extra_metadata.get('creatortool', PRODUCER),
           createdate=current_time, metadatadate=current_time,
           modifydate=current_time)

        if extra_metadata.get('title'):
            stream += '''
              <dc:title>
                <rdf:Alt>
                  <rdf:li xml:lang="x-default">{title}</rdf:li>
                </rdf:Alt>
              </dc:title>'''.format(title=extra_metadata.get('title'))

        # "An entity responsible for making the resource."
        # https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#http://purl.org/dc/terms/creator
        # So should be author...
        if extra_metadata.get('creator'):
            stream += '''
              <dc:creator>
                <rdf:Seq>
                  <rdf:li>{author}</rdf:li>
                </rdf:Seq>
              </dc:creator>'''.format(author=extra_metadata.get('author'))

        # TODO: Support multiple languages here?

        if extra_metadata.get('language'):
        # Empty language field means unknown language
            stream += '''
              <dc:language>
                <rdf:Bag>'''

            for language in extra_metadata.get('language', []):
                stream += '''
                  <rdf:li>{language}</rdf:li>'''.format(language=language)

            stream += '''
                </rdf:Bag>
              </dc:language>'''

        stream += '''
            </rdf:Description>
            <rdf:Description rdf:about="" xmlns:pdfaid="http://www.aiim.org/pdfa/ns/id/">
              <pdfaid:part>3</pdfaid:part>
              <pdfaid:conformance>B</pdfaid:conformance>
            </rdf:Description>
          </rdf:RDF>
        </x:xmpmeta>
        <?xpacket end="r"?>'''

        to_pdf.setXmlMetadata(stream)


#  pymupdf inserts stuff like '/Author (none)' when the author is not provided.
#  This is wrong. We'll file a bug, but let's first fix it here.
def fixup_pymupdf_metadata(doc):
    # Access to the Info xref is not in the API, so let's dig for it.
    trailer_lines = outdoc.PDFTrailer().split('\n')
    for line in trailer_lines:
        if '  /Info ' in line:
            s = line.replace('  /Info ', '')
            info_xref = s[:s.find(' ')]
            info_xref = int(info_xref)

            s = doc.xrefObject(info_xref)

            new_s = ''

            for infoline in s.split('\n'):
                if re.match('^.*\/[A-Za-z]+ \(none\)$', infoline):
                    continue

                new_s += infoline + '\n'

            doc.updateObject(info_xref, new_s)

            break


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(
            description='PDF recoder version %s.' % VERSION +
                        ' Compresses PDFs with images and inserts text layers '
                        ' based on hOCR input files.')
    parser.add_argument('-P', '--from-pdf', type=str, default=None,
                        help='Input PDF (containing images) to recode')
    parser.add_argument('-I', '--from-imagestack', type=str, default=None,
                        help='Glob pattern for image stack')
    parser.add_argument('-D', '--dpi', type=int, default=None,
                        help='DPI of input images, supply this to get '
                             'proportional page sizes in resulting PDF')
    parser.add_argument('-T', '--hocr-file', type=str, default=None,
                        help='hOCR file containing page information '
                              '(currently not optional)')
    parser.add_argument('-S', '--scandata-file', type=str, default=None,
                        help='archive.org specific.'
                              'Scandata XML file containing information on '
                              'which pages to skip (optional). This is helpful '
                              'if the input PDF is a PDF where certain '
                              'pages have already been skipped, but the hOCR '
                              'still has the pages in its file structure, '
                              'and is also used for page labels (numbering)')
    parser.add_argument('-o', '--out-pdf', type=str, default=None,
                        help='Output file to write recoded PDF to.')
    parser.add_argument('-O', '--out-dir', type=str, default=None,
                        help='Output directory to (also) write images to.')
    parser.add_argument('-R', '--reporter', type=str, default=None,
                        help='Program to launch when reporting progress.')
    parser.add_argument('-m', '--image-mode', default=IMAGE_MODE_MRC,
                        help='Compression mode. 0 is pass-through, 1 is pixmap'
                              ' 2 is MRC (default is 2). 3 is skip images',
                              type=int)
    parser.add_argument('--jbig2', default=False, help='Encode using jbig2',
                        action='store_true')
    parser.add_argument('-v', '--verbose', default=False, action='store_true',
                        help='Verbose output')
    parser.add_argument('--tmp-dir', default=None, type=str,
                        help='Directory to store temporary intermediate images')
    parser.add_argument('--report-every', default=None, type=int,
                        help='Briefly repor on status every N pages '
                             '(default is no reporting)')
    parser.add_argument('-t', '--stop-after', default=None, type=int,
                        help='Stop after N pages (default is no stop)')
    parser.add_argument('--bg-slope', default=47000, type=int,
                        help='Slope for background layer.'
                             ' Default is 47000')
    parser.add_argument('--fg-slope', default=49000, type=int,
                        help='Slope for foreground layer.'
                             ' Default is 49000')
    parser.add_argument('--bg-downsample', default=None, type=int,
                        help='Downsample background by factor.'
                             ' Default is no scaling')
    parser.add_argument('--denoise-mask', default=None,
                        help='Denoise mask when MRC algorithm thinks it needs'
                             ' to denoise - which is not always.'
                             ' Default is off', action='store_true')
    parser.add_argument('--hq-pages', type=str, default=None,
                        help='Pages to render in higher quality, provided '
                             'as comma separate values, negative indexing is '
                             'allowed, e.g.: --hq-pages \'1,2,3,4,-4,-3,-2,-1\''
                             ' will make the first four and last four pages '
                             'of a higher quality. Pages marked as higher '
                             'quality will not get downsampled and might use '
                             'different slope values (see --hq-bg-slope '
                             'and --hq-fg-slope)')
    parser.add_argument('--hq-bg-slope', default=47000, type=int,
                        help='High quality slope for background layer.'
                             ' Default is 47000')
    parser.add_argument('--hq-fg-slope', default=47000, type=int,
                        help='High quality slope for foreground layer.'
                             ' Default is 47000')
    parser.add_argument('--metadata-url', type=str, default=None,
                        help='URL describing document, if any')
    parser.add_argument('--metadata-title', type=str, default=None,
                        help='Title of PDF document')
    parser.add_argument('--metadata-author', type=str, default=None,
                        help='Author of document')
    parser.add_argument('--metadata-creator', type=str, default=None,
                        help='Creator of PDF document')
    parser.add_argument('--metadata-language', type=str, default=None,
                        nargs='+', action='extend',
                        help='Language of PDF document, see RFC 3066. '
                             'If multiple languages are specified, only the '
                             'first is added to the PDF catalog, but all of '
                             'them will end up in the XMP metadata')
    parser.add_argument('--metadata-subject', type=str, default=None,
                        help='Subjects')
    parser.add_argument('--metadata-creatortool', type=str, default=None,
                        help='Creator tool')


    args = parser.parse_args()
    if (args.from_pdf is None and args.from_imagestack is None) or args.out_pdf is None:
        sys.stderr.write('***** Error: --from-pdf or --out-pdf missing\n\n')
        parser.print_help()
        sys.exit(1)

    if args.from_imagestack is not None and args.from_pdf is not None:
        sys.stderr.write('***** Error: --from-pdf and --from-imagestack '
                         'are mutually exclusive\n\n')
        parser.print_help()
        sys.exit(1)

    in_pdf = None
    if args.from_pdf:
        in_pdf = fitz.open(args.from_pdf)

    image_files = None
    if args.from_imagestack:
        image_files = sorted(glob(args.from_imagestack))

    hocr_file = args.hocr_file
    outfile = args.out_pdf

    VERBOSE = args.verbose
    REPORT_EVERY = args.report_every
    STOP = args.stop_after
    if STOP is not None:
        STOP -= 1
    TMP_DIR = args.tmp_dir

    if VERBOSE:
        from numpy.core._multiarray_umath import __cpu_features__ as cpu_have
        cpu = cpu_have
        for k, v in cpu.items():
            if v:
                print('\t', k)


    reporter = args.reporter.split(' ') if args.reporter else None

    start_time = time()

    # Figure out if we have scandata, and figure out if we want to skip pages
    # based on scandata.
    skip_pages = []
    if args.scandata_file is not None:
        skip_pages = scandata_xml_get_skip_pages(args.scandata_file)

    # TODO: use a buffer, since it's typically quite small
    fd, tess_tmp_path = mkstemp(prefix='pdfrenderer', suffix='.pdf', dir=TMP_DIR)
    os.close(fd)

    if args.verbose:
        print('Creating text only PDF')

    # 1. Create text-only PDF from hOCR first, but honour page sizes of in_pdf
    create_tess_textonly_pdf(hocr_file, tess_tmp_path, in_pdf=in_pdf,
            image_files=image_files, dpi=args.dpi,
            skip_pages=skip_pages, reporter=reporter)

    if args.verbose:
        print('Inserting (and compressing) images')
    # 2. Load tesseract PDF and stick images in the PDF
    # We open the generated file but do not modify it in place
    outdoc = fitz.open(tess_tmp_path)

    HQ_PAGES = [False for x in range(outdoc.pageCount)]
    if args.hq_pages is not None:
        index_range = map(int, args.hq_pages.split(','))
        for i in index_range:
            # We want 0-indexed, not 1-indexed, but not negative numbers we want
            # to remain 1-indexed.
            if i > 0:
                i = i - 1

            if abs(i) >= len(HQ_PAGES):
                # Page out of range, silently ignore for automation purposes.
                # We don't want scripts that call out tool to worry about how
                # many a PDF has exactly. E.g. if 1,2,3,4,-4,-3,-2,-1 is passed,
                # and a PDF has only three pages, let's just set them all to HQ
                # and not complain about 4 and -4 being out of range.
                continue

            # Mark page as HQ
            HQ_PAGES[i] = True


    if VERBOSE:
        print('Converting with image mode:', args.image_mode)
    if args.image_mode == 2:
        insert_images_mrc(outdoc, hocr_file,
                          from_pdf=in_pdf,
                          image_files=image_files,
                          dpi=args.dpi,
                          bg_slope=args.bg_slope,
                          fg_slope=args.fg_slope,
                          skip_pages=skip_pages,
                          img_dir=args.out_dir,
                          jbig2=args.jbig2,
                          bg_downsample=args.bg_downsample,
                          denoise_mask=args.denoise_mask,
                          reporter=reporter,
                          hq_pages=HQ_PAGES,
                          hq_bg_slope=args.hq_bg_slope,
                          hq_fg_slope=args.hq_fg_slope)
    elif args.image_mode in (0, 1):
        insert_images(in_pdf, outdoc, mode=args.image_mode)
    elif args.image_mode == 3:
        # 3 = skip
        pass

    # 3. Add PDF/A compliant data
    write_pdfa(outdoc)

    if args.scandata_file is not None:
        # XXX: we parse scandata twice now, let's not do that
        write_page_labels(outdoc, args.scandata_file)


    lang_if_any = args.metadata_language[0] if args.metadata_language else None
    write_basic_ua(outdoc, language=lang_if_any)

    # 4. Write metadata
    extra_metadata = {}
    if args.metadata_url:
        extra_metadata['url'] = args.metadata_url
    if args.metadata_title:
        extra_metadata['title'] = args.metadata_title
    if args.metadata_creator:
        extra_metadata['creator'] = args.metadata_creator
    if args.metadata_author:
        extra_metadata['author'] = args.metadata_author
    if args.metadata_language:
        extra_metadata['language'] = args.metadata_language
    if args.metadata_subject:
        extra_metadata['subject'] = args.metadata_subject
    if args.metadata_creatortool:
        extra_metadata['creatortool'] = args.metadata_creatortool
    write_metadata(in_pdf, outdoc, extra_metadata=extra_metadata)

    print('Fixing up pymupdf metadata')
    fixup_pymupdf_metadata(outdoc)

    # 5. Save
    if VERBOSE:
        print('mupdf warnings, if any:', repr(fitz.TOOLS.mupdf_warnings()))
    if VERBOSE:
        print('Saving PDF now')

    t = time()
    outdoc.save(outfile, deflate=True, pretty=True)
    save_time_ms = int((time() - t)*1000)
    if reporter:
        data = json.dumps({'time_to_save': {'time': save_time_ms}})
        subprocess.check_output(reporter, input=data.encode('utf-8'))

    end_time = time()
    print('Processed %d pages at %.2f seconds/page' % (len(outdoc),
        (end_time - start_time) / len(outdoc)))

    if args.from_pdf is not None:
        oldsize = os.path.getsize(args.from_pdf)
    else:
        bytesum = 0
        skipped_pages = 0
        for idx, fname in enumerate(image_files):
            if skip_pages is not None and idx in skip_pages:
                skipped_pages += 1
                continue

            if args.stop_after is not None and (idx - skipped_pages) > args.stop_after:
                break

            bytesum += os.path.getsize(fname)

        oldsize = bytesum

    newsize = os.path.getsize(args.out_pdf)
    if VERBOSE:
        print('Compression ratio: %f' % (oldsize / newsize))

    # 5. Remove leftover files
    remove(tess_tmp_path)
