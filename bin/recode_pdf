#!/usr/bin/env python3
import pkg_resources

import sys
import os
import subprocess
from os import remove
from time import time
from datetime import datetime
from tempfile import mkstemp
from os.path import join
import shutil
import json

from PIL import Image
import fitz

from internetarchivepdf.mrc import KDU_EXPAND, create_mrc_components, create_mrc_hocr_components, \
        encode_mrc_images
from internetarchivepdf.pdfrenderer import TessPDFRenderer
from internetarchivepdf.scandata import scandata_xml_get_skip_pages
from hocr.parse import (hocr_page_iterator, hocr_page_to_word_data,
        hocr_page_get_dimensions)


VERSION = '0.0.1'

# XXX: More stable URL?
SOFTWARE_URL = 'https://git.archive.org/merlijn/archive-pdf-tools'
PRODUCER = 'Internet Archive PDF creator and recoder %s; %s; written by '\
           'Merlijn B. W. Wajer. Powered by Tesseract, mupdf and '\
           'Python (pymupdf/skimage).' % (VERSION, SOFTWARE_URL)


STOP = None
VERBOSE = False
REPORT_EVERY = None
TMP_DIR = None

IMAGE_MODE_PASSTHROUGH = 0
IMAGE_MODE_PIXMAP = 1
IMAGE_MODE_MRC = 2


def create_tess_textonly_pdf(in_pdf, hocr_file, save_path, skip_pages=None,
                             reporter=None):
    hocr_iter = hocr_page_iterator(hocr_file)

    render = TessPDFRenderer()
    render.BeginDocumentHandler()

    skipped_pages = 0

    last_time = time()
    reporting_page_count = 0

    for idx, hocr_page in enumerate(hocr_iter):
        w, h = hocr_page_get_dimensions(hocr_page)

        if skip_pages is not None and idx in skip_pages:
            if VERBOSE:
                print('Skipping page %d' % idx)
            skipped_pages += 1
            continue

        if STOP is not None and (idx - skipped_pages) >= STOP:
            break

        page = in_pdf[idx - skipped_pages]

        width = page.rect.width
        height = page.rect.height

        scaler = page.rect.width / w
        ppi = 72 / scaler

        word_data = hocr_page_to_word_data(hocr_page, scaler)
        render.AddImageHandler(word_data, width, height, ppi=ppi)

        reporting_page_count += 1


    if reporter and reporting_page_count != 0:
        current_time = time()
        ms = int(((current_time - last_time) / reporting_page_count) * 1000)

        data = json.dumps({'text_pages': {'count': reporting_page_count,
                                              'time-per': ms}})
        subprocess.check_output(reporter, input=data.encode('utf-8'))

    render.EndDocumentHandler()

    fp = open(save_path, 'wb+')
    fp.write(render._data)
    fp.close()


def insert_images_mrc(from_pdf, to_pdf, hocr_file, bg_slope=None, fg_slope=None,
        skip_pages=None, img_dir=None, jbig2=False, bg_downsample=None,
        reporter=None):
    hocr_iter = hocr_page_iterator(hocr_file)

    skipped_pages = 0

    last_time = time()
    reporting_page_count = 0

    #for idx, page in enumerate(to_pdf):
    for idx, hocr_page in enumerate(hocr_iter):
        if skip_pages is not None and idx in skip_pages:
            if VERBOSE:
                print('IMAGES Skipping page %d' % idx)
            skipped_pages += 1
            continue

        idx = idx - skipped_pages

        if STOP is not None and idx >= STOP:
            break

        page = to_pdf[idx]

        # XXX: TODO: FIXME: MEGAHACK: For some reason the _imgonly PDFs
        # generated by us have all images on all pages according to pymupdf, so
        # hack around that for now.
        img = sorted(from_pdf.getPageImageList(idx))[idx]
        #img = from_pdf.getPageImageList(idx)[0]

        xref = img[0]
        maskxref = img[1]
        # TODO: Do not assume JPX/JPEG2000 here, probe for image format
        image = from_pdf.extractImage(xref)
        jpx = image["image"]

        fd, jpx_in = mkstemp(prefix='in', suffix='.jpx', dir=TMP_DIR)
        os.write(fd, jpx)
        os.close(fd)

        fd, tiff_in = mkstemp(prefix='in', suffix='.tiff', dir=TMP_DIR)
        os.close(fd)
        os.remove(tiff_in)

        subprocess.check_call([KDU_EXPAND, '-i', jpx_in, '-o',
            tiff_in], stderr=subprocess.DEVNULL,
            stdout=subprocess.DEVNULL)
        os.remove(jpx_in)

        hocr_word_data = hocr_page_to_word_data(hocr_page)
        mask, bg, fg = create_mrc_hocr_components(Image.open(tiff_in),
                                                  hocr_word_data,
                                                  bg_downsample=bg_downsample)
        remove(tiff_in)

        mask_f, bg_f, fg_f = encode_mrc_images(mask, bg, fg,
                bg_slope=bg_slope, fg_slope=fg_slope,
                tmp_dir=TMP_DIR, jbig2=jbig2)

        if img_dir is not None:
            shutil.copy(mask_f, join(img_dir, '%.6d_mask.jbig2' % idx))
            shutil.copy(bg_f, join(img_dir, '%.6d_bg.jp2' % idx))
            shutil.copy(fg_f, join(img_dir, '%.6d_fg.jp2' % idx))

        bg_contents = open(bg_f, 'rb').read()
        page.insertImage(page.rect, stream=bg_contents, mask=None,
                overlay=False)

        fg_contents = open(fg_f, 'rb').read()
        mask_contents = open(mask_f, 'rb').read()

        page.insertImage(page.rect, stream=fg_contents, mask=mask_contents,
                overlay=True)

        # Remove leftover files
        remove(mask_f)
        remove(bg_f)
        remove(fg_f)

        reporting_page_count += 1

        if REPORT_EVERY is not None and reporting_page_count % REPORT_EVERY == 0:
            print('Processed %d PDF pages.' % idx)
            sys.stdout.flush()

            if reporter:
                current_time = time()
                ms = int(((current_time - last_time) / reporting_page_count) * 1000)

                data = json.dumps({'compress_pages': {'count': reporting_page_count,
                                                 'time-per': ms}})
                subprocess.check_output(reporter, input=data.encode('utf-8'))

                # Reset chunk timer
                last_time = time()
                # Reset chunk counter
                reporting_page_count = 0


    if reporter and reporting_page_count != 0:
        current_time = time()
        ms = int(((current_time - last_time) / reporting_page_count) * 1000)

        data = json.dumps({'text_pages': {'count': reporting_page_count,
                                         'time-per': ms}})
        subprocess.check_output(reporter, input=data.encode('utf-8'))


def insert_images(from_pdf, to_pdf, mode):
    # TODO: implement img_dir here

    for idx, page in enumerate(to_pdf):
        # XXX: TODO: FIXME: MEGAHACK: For some reason the _imgonly PDFs
        # generated by us have all images on all pages according to pymupdf, so
        # hack around that for now.
        img = sorted(from_pdf.getPageImageList(idx))[idx]
        #img = from_pdf.getPageImageList(idx)[0]

        xref = img[0]
        maskxref = img[1]
        if mode == IMAGE_MODE_PASSTHROUGH:
            image = from_pdf.extractImage(xref)
            page.insertImage(page.rect, stream=image["image"], overlay=False)
        elif mode == IMAGE_MODE_PIXMAP:
            pixmap = fitz.Pixmap(from_pdf, xref)
            page.insertImage(page.rect, pixmap=pixmap, overlay=False)

        if STOP is not None and idx >= STOP:
            break

        if REPORT_EVERY is not None and idx % REPORT_EVERY == 0:
            print('Processed %d PDF pages.' % idx)
            sys.stdout.flush()


# XXX: tmp.icc - pick proper one and ship it with the tool, or embed it
def write_pdfa(to_pdf):
    srgbxref = to_pdf._getNewXref()
    to_pdf.updateObject(srgbxref, """
<<
      /Alternate /DeviceRGB
      /N 3
>>
""")
    icc = pkg_resources.resource_string('internetarchivepdf', "data/tmp.icc")
    to_pdf.updateStream(srgbxref, icc, new=True)

    intentxref = to_pdf._getNewXref()
    to_pdf.updateObject(intentxref, """
<<
  /Type /OutputIntent
  /S /GTS_PDFA1
  /OutputConditionIdentifier (Custom)
  /Info (sRGB IEC61966-2.1)
  /DestOutputProfile %d 0 R
>>
""" % srgbxref)

    catalogxref = to_pdf.PDFCatalog()
    s = to_pdf.xrefObject(to_pdf.PDFCatalog())
    s = s[:-2]
    s += '  /OutputIntents [ %d 0 R ]' % intentxref
    s += '>>'
    to_pdf.updateObject(catalogxref, s)


def write_metadata(from_pdf, to_pdf, extra_metadata):
    doc_md = from_pdf.metadata if from_pdf is not None else {}

    doc_md['producer'] = PRODUCER

    if 'url' in extra_metadata:
        doc_md['keywords'] = extra_metadata['url']
    if 'title' in extra_metadata:
        doc_md['title'] = extra_metadata['title']
    if 'creator' in extra_metadata:
        doc_md['creator'] = extra_metadata['creator']
    if 'subject' in extra_metadata:
        doc_md['subject'] = extra_metadata['subject']

    current_time = 'D:' + datetime.utcnow().isoformat(timespec='seconds') + '+00\':00\''
    if from_pdf is not None:
        doc_md['creationDate'] = from_pdf.metadata['creationDate']
    else:
        doc_md['creationDate'] = current_time
    doc_md['modDate'] = current_time

    # Set PDF basic metadata
    to_pdf.setMetadata(doc_md)

    have_xmlmeta = (from_pdf is not None) and (from_pdf._getXmlMetadataXref() > 0)
    if have_xmlmeta:
        xml_xref = from_pdf._getXmlMetadataXref()

        # Just copy the existing XML, perform no validity checks
        xml_bytes = from_pdf.xrefStream(xml_xref)
        to_pdf.setXmlMetadata(xml_bytes.decode('utf-8'))
    else:
        # TODO: Update this and make sure it's all nice and correct
        # TODO: Write/add:
        # - Language bag
        # - *link back* to archive.org item

        current_time = datetime.utcnow().isoformat(timespec='seconds') + '+00:00'

        stream='''<?xpacket begin="..." id="W5M0MpCehiHzreSzNTczkc9d"?>
        <x:xmpmeta xmlns:x="adobe:ns:meta/">
          <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
            <rdf:Description rdf:about="" xmlns:xmp="http://ns.adobe.com/xap/1.0/">
              <xmp:CreateDate>{createdate}</xmp:CreateDate>
              <xmp:MetadataDate>{metadatadate}</xmp:MetadataDate>
              <xmp:ModifyDate>{modifydate}</xmp:ModifyDate>
              <xmp:CreatorTool>{creatortool}</xmp:CreatorTool>
            </rdf:Description>
            <rdf:Description rdf:about="" xmlns:dc="http://purl.org/dc/elements/1.1/">
              <dc:title>
                <rdf:Alt>
                  <rdf:li xml:lang="x-default">{title}</rdf:li>
                </rdf:Alt>
              </dc:title>
              <dc:creator>
                <rdf:Seq>
                  <rdf:li>{creator}</rdf:li>
                </rdf:Seq>
              </dc:creator>
              <dc:language>
                <rdf:Bag>
                  <rdf:li>{language}</rdf:li>
                </rdf:Bag>
              </dc:language>
            </rdf:Description>
            <rdf:Description rdf:about="" xmlns:pdfaid="http://www.aiim.org/pdfa/ns/id/">
              <pdfaid:part>3</pdfaid:part>
              <pdfaid:conformance>B</pdfaid:conformance>
            </rdf:Description>
          </rdf:RDF>
        </x:xmpmeta>
        <?xpacket end="r"?>'''.format(
                # XXX: We really just want to omit this metadata alltogether if it's
                # not provided
                title=extra_metadata.get('title', 'No title provided'),
                creator=extra_metadata.get('creator', 'No creator provided'),
                creatortool=extra_metadata.get('creatortool', 'No creator tool provided'),
                # Empty language field means unknown language
                language=extra_metadata.get('language', ''),
                createdate=current_time,
                metadatadate=current_time,
                modifydate=current_time)

        to_pdf.setXmlMetadata(stream)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(
            description='PDF recoder version %s.' % VERSION +
                        ' Compresses PDFs with images and inserts text layers '
                        ' based on hOCR input files.')
    parser.add_argument('-i', '--from-pdf', type=str, default=None,
                        help='Input PDF (containing images) to recode')
    parser.add_argument('-T', '--hocr-file', type=str, default=None,
                        help='hOCR file containing page information '
                              '(currently not optional)')
    parser.add_argument('-S', '--scandata-file', type=str, default=None,
                        help='Scandata XML file containing information on '
                              'which pages to skip (optional). This is helpful '
                              'if the input PDF is a PDF where certain '
                              'pages have already been skipped, but the hOCR '
                              'still has the pages in its file structure.')
    parser.add_argument('-o', '--out-pdf', type=str, default=None,
                        help='Output file to write recoded PDF to.')
    parser.add_argument('-O', '--out-dir', type=str, default=None,
                        help='Output directory to (also) write images to.')
    parser.add_argument('-R', '--reporter', type=str, default=None,
                        help='Program to launch when reporting progress.')
    parser.add_argument('-m', '--image-mode', default=IMAGE_MODE_MRC,
                        help='Compression mode. 0 is pass-through, 1 is pixmap'
                              ' 2 is MRC (default is 2)', type=int)
    parser.add_argument('--jbig2', default=False, help='Encode using jbig2',
                        action='store_true')
    parser.add_argument('-v', '--verbose', default=False, action='store_true',
                        help='Verbose output')
    parser.add_argument('--tmp-dir', default=None, type=str,
                        help='Directory to store temporary intermediate images')
    parser.add_argument('--report-every', default=None, type=int,
                        help='Briefly repor on status every N pages '
                             '(default is no reporting)')
    parser.add_argument('-t', '--stop-after', default=None, type=int,
                        help='Stop after N pages (default is no stop)')
    parser.add_argument('--bg-slope', default=47000, type=int,
                        help='Slope for background layer.'
                             ' Default is 0.1')
    parser.add_argument('--fg-slope', default=49000, type=int,
                        help='Slope for foreground layer.'
                             ' Default is 0.05')
    parser.add_argument('--bg-downsample', default=None, type=int,
                        help='Downsample background by factor.'
                             ' Default is no scaling')
    parser.add_argument('--metadata-url', type=str, default=None,
                        help='URL describing document, if any')
    parser.add_argument('--metadata-title', type=str, default=None,
                        help='Title of PDF document')
    parser.add_argument('--metadata-creator', type=str, default=None,
                        help='Creator of PDF document')
    parser.add_argument('--metadata-language', type=str, default=None,
                        help='Language of PDF document, see RFC 3066')
    parser.add_argument('--metadata-subject', type=str, default=None,
                        help='Subjects')
    parser.add_argument('--metadata-creatortool', type=str, default=None,
                        help='Creator tool')


    args = parser.parse_args()
    if args.from_pdf is None or args.out_pdf is None:
        sys.stderr.write('***** Error: --from-pdf or --out-pdf missing\n\n')
        parser.print_help()
        sys.exit(1)

    in_pdf = fitz.open(args.from_pdf)
    hocr_file = args.hocr_file
    outfile = args.out_pdf

    VERBOSE = args.verbose
    REPORT_EVERY = args.report_every
    STOP = args.stop_after
    if STOP is not None:
        STOP -= 1
    TMP_DIR = args.tmp_dir

    reporter = args.reporter.split(' ') if args.reporter else None

    start_time = time()

    # Figure out if we have scandata, and figure out if we want to skip pages
    # based on scandata.
    skip_pages = []
    if args.scandata_file is not None:
        skip_pages = scandata_xml_get_skip_pages(args.scandata_file)

    # TODO: use a buffer, since it's typically quite small
    fd, tess_tmp_path = mkstemp(prefix='pdfrenderer', suffix='.pdf', dir=TMP_DIR)
    os.close(fd)

    if args.verbose:
        print('Creating text only PDF')

    # 1. Create text-only PDF from hOCR first, but honour page sizes of in_pdf
    create_tess_textonly_pdf(in_pdf, hocr_file, tess_tmp_path,
            skip_pages=skip_pages, reporter=reporter)

    if args.verbose:
        print('Inserting (and compressing) images')
    # 2. Load tesseract PDF and stick images in the PDF
    # We open the generated file but do not modify it in place
    outdoc = fitz.open(tess_tmp_path)

    if VERBOSE:
        print('Converting with image mode:', args.image_mode)
    if args.image_mode == 2:
        insert_images_mrc(in_pdf, outdoc, hocr_file,
                          bg_slope=args.bg_slope,
                          fg_slope=args.fg_slope, skip_pages=skip_pages,
                          img_dir=args.out_dir,
                          jbig2=args.jbig2,
                          bg_downsample=args.bg_downsample,
                          reporter=reporter)
    else:
        insert_images(in_pdf, outdoc, mode=args.image_mode)

    # 3. Add PDF/A compliant data
    write_pdfa(outdoc)

    # 4. Write metadata
    extra_metadata = {}
    if args.metadata_url:
        extra_metadata['url'] = args.metadata_url
    if args.metadata_title:
        extra_metadata['title'] = args.metadata_title
    if args.metadata_creator:
        extra_metadata['creator'] = args.metadata_creator
    if args.metadata_language:
        extra_metadata['language'] = args.metadata_language
    if args.metadata_subject:
        extra_metadata['subject'] = args.metadata_subject
    if args.metadata_creatortool:
        extra_metadata['creatortool'] = args.metadata_creatortool
    write_metadata(in_pdf, outdoc, extra_metadata=extra_metadata)

    # 5. Save
    if VERBOSE:
        print('mupdf warnings, if any:', repr(fitz.TOOLS.mupdf_warnings()))
    if VERBOSE:
        print('Saving PDF now')
    outdoc.save(outfile, deflate=True, pretty=True)

    end_time = time()
    print('Processed %d pages at %.2f seconds/page' % (len(outdoc),
        (end_time - start_time) / len(outdoc)))

    oldsize = os.path.getsize(args.from_pdf)
    newsize = os.path.getsize(args.out_pdf)
    if VERBOSE:
        print('Compression ratio: %f%%' % (oldsize / newsize))

    # 5. Remove leftover files
    remove(tess_tmp_path)
